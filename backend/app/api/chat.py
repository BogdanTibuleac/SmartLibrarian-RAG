from fastapi import APIRouter, Body
from app.rag.chroma_setup import collection
from app.rag.embeddings import get_embedding
from app.tools.moderation import is_prompt_flagged
from app.tools.distance import normalize_distances

# ‚¨áÔ∏è NEW: cache helpers
from app.db.db import (
    normalize_prompt,
    cache_lookup_exact,
    cache_lookup_fuzzy,
    cache_upsert,
)

import os
import openai
from dotenv import load_dotenv

MAX_ACCEPTABLE_RAW_DISTANCE = 1.19  
FUZZY_THRESHOLD = 0.70              # trigram similarity threshold

router = APIRouter()
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# Pricing via env (USD per 1K tokens) to avoid stale hardcodes
IN_PRICE = float(os.getenv("OPENAI_INPUT_PRICE_PER_1K", "0.0005"))
OUT_PRICE = float(os.getenv("OPENAI_OUTPUT_PRICE_PER_1K", "0.0015"))

def log_valid_results(results):
    print("\nüìä Top Matches:")
    for i, (doc, meta, raw_dist, norm_dist) in enumerate(results):
        print(f"üìö #{i+1}: {meta.get('title', 'N/A')}")
        print(f"üìè Raw distance: {raw_dist:.4f} | Normalized: {norm_dist:.4f}")
        print(f"üìù Summary (trimmed): {doc[:100]}...\n")


@router.post("/")
async def get_book_recommendation(query: str = Body(..., embed=True)):
    # Basic validation + moderation
    if not query or len(query.strip()) < 3:
        return {"error": "Interogare prea scurtƒÉ. Te rog reformuleazƒÉ."}

    if is_prompt_flagged(query):
        return {
            "recommended_title": None,
            "explanation": "Te rog formuleazƒÉ √Æntrebarea √Æntr-un mod respectuos. √é»õi stau la dispozi»õie cu recomandƒÉri literare.",
            "source_summary": None
        }

    # ---------- CACHE: exact then fuzzy ----------
    prompt_norm = normalize_prompt(query)

    # 1) exact cache
    hit = await cache_lookup_exact(prompt_norm)
    if hit:
        return {
            "recommended_title": None,                # you can fill from metadata if you later store it
            "explanation": hit["output_data"],        # cached final text
            "source_summary": None,                   # optional to store later
            "from_cache": True,
            "model_name": hit["model_name"],
            "generation_cost_usd": float(hit["generation_cost_usd"]),
            "generated_at": hit["generated_at"]
        }

    # 2) fuzzy fallback (pg_trgm)
    near = await cache_lookup_fuzzy(prompt_norm, threshold=FUZZY_THRESHOLD)
    if near:
        near["generation_cost_usd"] = float(near["generation_cost_usd"])
        return {
            "recommended_title": None,
            "explanation": near["output_data"],
            "source_summary": None,
            "from_cache": True,
            "model_name": near["model_name"],
            "generation_cost_usd": near["generation_cost_usd"],
            "generated_at": near["generated_at"]
        }

    # ---------- RAG: find top doc ----------
    print(f"üîç Query: {query}")
    query_embedding = get_embedding(query)

    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=3,
        include=["documents", "metadatas", "distances"]
    )

    documents = results["documents"][0]
    metadatas = results["metadatas"][0]
    distances = results["distances"][0]
    normalized_distances = normalize_distances(distances)

    valid_results = []
    for i, (doc, meta, raw_dist, norm_dist) in enumerate(zip(documents, metadatas, distances, normalized_distances)):
        if doc and meta:
            valid_results.append((doc, meta, raw_dist, norm_dist))

    if not valid_results:
        # Miss in RAG too
        return {
            "recommended_title": None,
            "explanation": "√émi pare rƒÉu, dar nu am gƒÉsit nicio carte potrivitƒÉ √Æn baza de date curentƒÉ.",
            "source_summary": None,
            "from_cache": False,
            "model_name": None,
            "generation_cost_usd": 0.0
        }

    log_valid_results(valid_results)

    # Use only the top result for the GPT explanation
    top_doc, top_meta, raw_dist, norm_dist = valid_results[0]

    if raw_dist > MAX_ACCEPTABLE_RAW_DISTANCE:
        print(f"‚ö†Ô∏è Top result too far (raw distance: {raw_dist:.4f}) ‚Äî skipping GPT.")
        return {
            "recommended_title": None,
            "explanation": "√émi pare rƒÉu, dar nu am gƒÉsit nicio carte relevantƒÉ pentru √Æntrebarea ta.",
            "source_summary": None,
            "normalized_distance": norm_dist,
            "from_cache": False,
            "model_name": None,
            "generation_cost_usd": 0.0
        }

    # ---------- LLM generation ----------
    model_name = "gpt-3.5-turbo"  # keep aligned with your account
    prompt = f"""
»öin√¢nd cont de interogarea: "{query}", oferƒÉ o recomandare pe baza urmƒÉtoarei cƒÉr»õi, care este cea mai apropiatƒÉ semantic dintre toate cele din baza de date (scor: {norm_dist:.4f}).

Titlu: {top_meta.get("title", "N/A")}
Rezumat: {top_doc.strip()}

Scrie o recomandare prietenoasƒÉ, clarƒÉ »ôi scurtƒÉ. ExplicƒÉ de ce aceastƒÉ carte rƒÉspunde cerin»õei utilizatorului, fƒÉrƒÉ a inventa alte op»õiuni.
Raspunsul trebuie sa fie de maxim 50 de cuvinte.
""".strip()

    # Synchronous OpenAI call is fine here; if needed, offload to threadpool later.
    response = openai.ChatCompletion.create(
        model=model_name,
        messages=[{"role": "user", "content": prompt}],
        max_tokens=200
    )

    gpt_reply = response["choices"][0]["message"]["content"].strip()

    # Cost calculation from usage (avoid hardcoding prices here; use env)
    usage = response.get("usage", {}) or {}
    prompt_tokens = usage.get("prompt_tokens", 0)
    completion_tokens = usage.get("completion_tokens", 0)
    generation_cost_usd = (prompt_tokens / 1000.0) * IN_PRICE + (completion_tokens / 1000.0) * OUT_PRICE

    print("üß† GPT Explanation:\n", gpt_reply)
    print(f"üí∞ Tokens in/out: {prompt_tokens}/{completion_tokens} -> cost ${generation_cost_usd:.6f}")

    # ---------- Persist to cache ----------
    await cache_upsert(
        prompt_norm,
        output_format="text",
        output_data=gpt_reply,
        model_name=model_name,
        generation_cost_usd=generation_cost_usd,
    )

    # ---------- Response ----------
    return {
        "recommended_title": top_meta.get("title"),
        "explanation": gpt_reply,
        "source_summary": top_doc,
        "normalized_distance": norm_dist,
        "from_cache": False,
        "model_name": model_name,
        "generation_cost_usd": generation_cost_usd
    }
